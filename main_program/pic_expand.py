import os
import numpy as np
import cv2
import torch
import shutil
from PIL import Image, ImageFilter
from utils.deep_learning import RtmlibPoseGet

from diffusers import (
    ControlNetModel,
    StableDiffusionXLControlNetInpaintPipeline,
    AutoencoderKL,
    StableDiffusionXLInpaintPipeline,
)
from diffusers.utils import load_image
from utils.config import get_default_config


# =========================
# 1. è®¡ç®— 16:10 ç”»å¸ƒ
# =========================
def _ceil_to(x: int, base: int) -> int:
    return ((x + base - 1) // base) * base


def calculate_wh(h: int, w: int, ratio: float = 10 / 16, align: int = 64):
    if h / w < ratio:
        new_w = w
        new_h = int(round(w * ratio))
    else:
        new_h = h
        new_w = int(round(h / ratio))

    new_w = _ceil_to(new_w, align)
    new_h = _ceil_to(new_h, align)
    return new_h, new_w


# =========================
# 2. åˆ›å»ºç”»å¸ƒ + æ‰©å±•åŒºåŸŸ Mask
# =========================
def create_canvas_and_mask(img_path, feather_radius=12):
    image = load_image(img_path).convert("RGB")
    w, h = image.size

    new_h, new_w = calculate_wh(h, w, ratio=10 / 16, align=64)
    p_x = (new_w - w) // 2
    p_y = (new_h - h) // 2

    canvas = Image.new("RGB", (new_w, new_h), (0, 0, 0))
    canvas.paste(image, (p_x, p_y))

    # mask: ç™½è‰²=é‡ç»˜åŒºåŸŸ, é»‘è‰²=ä¿ç•™åŒºåŸŸ
    mask = Image.new("L", (new_w, new_h), 255)
    keep = Image.new("L", (w, h), 0)
    mask.paste(keep, (p_x, p_y))

    # ç¾½åŒ–è¾¹ç¼˜
    mask = mask.filter(ImageFilter.GaussianBlur(radius=feather_radius))

    return canvas, mask, (p_x, p_y)


# =========================
# 3. Stage 0ï¼šæå– Pose + äººç‰© Mask
# =========================
def stage0_extract_pose_and_mask(img_path, pose_getter, padding=20):
    """
    æå–äººç‰©éª¨æ¶å›¾ + äººç‰©åŒºåŸŸ mask
    è¿”å›ï¼š
        - pose_image: éª¨æ¶å›¾ (PIL RGB)
        - person_mask: äººç‰©åŒºåŸŸ mask (PIL L)ï¼Œç™½è‰²=äººç‰©
    """
    image = load_image(img_path).convert("RGB")
    img_np = np.array(image)

    # è·å–å…³é”®ç‚¹
    keypoints, scores = pose_getter.get_keypoints(img_np)

    if keypoints is None or len(keypoints) == 0:
        print("âš ï¸ æœªæ£€æµ‹åˆ°äººç‰©ï¼Œè¿”å›ç©º mask")
        w, h = image.size
        return Image.new("RGB", (w, h), (0, 0, 0)), Image.new("L", (w, h), 0)

    h, w = img_np.shape[:2]

    # 1. ç»˜åˆ¶éª¨æ¶å›¾
    pose_canvas = np.zeros((h, w, 3), dtype=np.uint8)

    # OpenPose è¿æ¥å…³ç³»ï¼ˆCOCO 17ç‚¹ï¼‰
    connections = [
        (0, 1),
        (0, 2),
        (1, 3),
        (2, 4),  # å¤´éƒ¨
        (5, 6),
        (5, 7),
        (7, 9),
        (6, 8),
        (8, 10),  # ä¸ŠåŠèº«
        (5, 11),
        (6, 12),
        (11, 12),  # èº¯å¹²
        (11, 13),
        (13, 15),
        (12, 14),
        (14, 16),  # ä¸‹åŠèº«
    ]

    # ç”»éª¨æ¶çº¿
    for kp in keypoints:
        for start_idx, end_idx in connections:
            if start_idx < len(kp) and end_idx < len(kp):
                start = tuple(kp[start_idx].astype(int))
                end = tuple(kp[end_idx].astype(int))
                if scores[0][start_idx] > 0.3 and scores[0][end_idx] > 0.3:
                    cv2.line(pose_canvas, start, end, (255, 255, 255), 3)

    # ç”»å…³é”®ç‚¹
    for kp in keypoints:
        for idx, point in enumerate(kp):
            if scores[0][idx] > 0.3:
                cv2.circle(pose_canvas, tuple(point.astype(int)), 4, (0, 255, 0), -1)

    pose_image = Image.fromarray(pose_canvas)

    # 2. åˆ›å»ºäººç‰© mask
    person_mask = np.zeros((h, w), dtype=np.uint8)

    for kp in keypoints:
        valid_points = []
        for idx, point in enumerate(kp):
            if scores[0][idx] > 0.3:
                valid_points.append(point.astype(int))

        if len(valid_points) > 0:
            valid_points = np.array(valid_points)
            x_min = max(0, valid_points[:, 0].min() - padding)
            x_max = min(w, valid_points[:, 0].max() + padding)
            y_min = max(0, valid_points[:, 1].min() - padding)
            y_max = min(h, valid_points[:, 1].max() + padding)

            person_mask[y_min:y_max, x_min:x_max] = 255

    # å½¢æ€å­¦é—­è¿ç®—ï¼Œå¡«è¡¥äººç‰©åŒºåŸŸç©ºæ´
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (30, 30))
    person_mask = cv2.morphologyEx(person_mask, cv2.MORPH_CLOSE, kernel)

    person_mask_pil = Image.fromarray(person_mask)

    return pose_image, person_mask_pil



# =========================
# 5. Canny Control
# =========================
def make_canny_control(image: Image.Image, low=120, high=220, blur_sigma=0.8):
    np_img = np.array(image)
    gray = cv2.cvtColor(np_img, cv2.COLOR_RGB2GRAY)

    if blur_sigma and blur_sigma > 0:
        gray = cv2.GaussianBlur(gray, (0, 0), sigmaX=blur_sigma, sigmaY=blur_sigma)

    edges = cv2.Canny(gray, threshold1=low, threshold2=high)
    edges_rgb = np.stack([edges, edges, edges], axis=2)
    return Image.fromarray(edges_rgb, mode="RGB")


# =========================
# 6. åŠ è½½ Pipelines
# =========================
def load_inpaint_pipe(base_model, vae_model):
    vae = AutoencoderKL.from_pretrained(
        vae_model,
        torch_dtype=torch.float16,
        use_safetensors=True,
    )
    pipe = StableDiffusionXLInpaintPipeline.from_pretrained(
        base_model,
        vae=vae,
        torch_dtype=torch.float16,
        use_safetensors=True,
    )
    pipe.enable_sequential_cpu_offload()
    return pipe


def load_controlnet_inpaint_pipe(base_model, control_model, vae_model):
    controlnet = ControlNetModel.from_pretrained(
        control_model,
        torch_dtype=torch.float16,
        use_safetensors=True,
    )
    vae = AutoencoderKL.from_pretrained(
        vae_model,
        torch_dtype=torch.float16,
        use_safetensors=True,
    )
    pipe = StableDiffusionXLControlNetInpaintPipeline.from_pretrained(
        base_model,
        controlnet=controlnet,
        vae=vae,
        torch_dtype=torch.float16,
        use_safetensors=True,
    )
    pipe.enable_sequential_cpu_offload()
    return pipe


# =========================
# 7. Stage 1ï¼šbase_model inpaint æ‰©å±•èƒŒæ™¯
# =========================
def stage1_base_inpaint(
    pipe_inpaint,
    img_path,
    pose_image,
    person_mask,
    prompt,
    negative_prompt,
    feather_radius=12,
    guidance_scale=7.5,
    num_inference_steps=30,
    strength=1.0,
):
    """
    ç”¨ base_model çš„ inpaint æ‰©å±•èƒŒæ™¯
    ä¿æŠ¤äººç‰©åŒºåŸŸä¸è¢«é‡ç»˜
    """
    canvas, expand_mask, paste_pos = create_canvas_and_mask(
        img_path, feather_radius=feather_radius
    )

    # æ‰©å±• pose å’Œ mask åˆ°æ–°ç”»å¸ƒ
    new_w, new_h = canvas.size
    pose_canvas = expand_to_canvas(pose_image, (new_w, new_h), paste_pos, color=255)
    person_mask_canvas = expand_to_canvas(person_mask, (new_w, new_h), paste_pos, color=0)

    # åˆå¹¶ maskï¼šåªé‡ç»˜æ‰©å±•åŒºåŸŸï¼Œä¸ç¢°äººç‰©
    expand_np = np.array(expand_mask)
    person_np = np.array(person_mask_canvas)
    safe_mask_np = np.where(person_np > 128, 0, expand_np)
    safe_mask = Image.fromarray(safe_mask_np.astype(np.uint8))

    print(f"Stage1: Base Inpaint æ‰©å±•èƒŒæ™¯ {new_w}x{new_h}ï¼ˆä¿æŠ¤äººç‰©ï¼‰...")
    result = pipe_inpaint(
        prompt=prompt,
        negative_prompt=negative_prompt,
        image=canvas,
        mask_image=safe_mask,
        guidance_scale=guidance_scale,
        num_inference_steps=num_inference_steps,
        strength=strength,
        width=new_w,
        height=new_h,
    ).images[0]

    return result, expand_mask, person_mask_canvas, pose_canvas


# =========================
# 8. Stage 2ï¼šCanny é‡æ„èƒŒæ™¯
# =========================
def stage2_canny_background(
    pipe_control,
    base_image: Image.Image,
    expand_mask: Image.Image,
    person_mask: Image.Image,
    prompt,
    negative_prompt,
    controlnet_conditioning_scale=0.75,
    guidance_scale=6.0,
    num_inference_steps=35,
    strength=0.85,
    canny_low=120,
    canny_high=220,
    canny_blur_sigma=0.8,
    debug_canny_path=None,
):
    w, h = base_image.size

    # ç”Ÿæˆ Canny æ§åˆ¶å›¾
    canny_control = make_canny_control(
        base_image, low=canny_low, high=canny_high, blur_sigma=canny_blur_sigma
    )

    if debug_canny_path:
        os.makedirs(os.path.dirname(debug_canny_path) or ".", exist_ok=True)
        canny_control.save(debug_canny_path)

    # mask: åªé‡ç»˜æ‰©å±•åŒºåŸŸï¼ˆä¸ç¢°äººç‰©ï¼‰
    expand_np = np.array(expand_mask)
    person_np = np.array(person_mask)
    background_mask_np = np.where(person_np > 128, 0, expand_np)
    background_mask = Image.fromarray(background_mask_np.astype(np.uint8))

    print(f"Stage2: Canny ControlNet é‡æ„èƒŒæ™¯ {w}x{h}...")
    result = pipe_control(
        prompt=prompt,
        negative_prompt=negative_prompt,
        image=base_image,
        mask_image=background_mask,
        control_image=canny_control,
        controlnet_conditioning_scale=controlnet_conditioning_scale,
        guidance_scale=guidance_scale,
        num_inference_steps=num_inference_steps,
        strength=strength,
        width=w,
        height=h,
    ).images[0]

    return result


# =========================
# 9. Stage 3ï¼šOpenPose ä¿®æ­£äººç‰©
# =========================
def stage3_openpose_refine_person(
    pipe_control,
    base_image: Image.Image,
    person_mask: Image.Image,
    pose_control: Image.Image,
    prompt,
    negative_prompt,
    controlnet_conditioning_scale=0.85,
    guidance_scale=6.5,
    num_inference_steps=40,
    strength=0.75,
    debug_pose_path=None,
):
    w, h = base_image.size

    if debug_pose_path:
        os.makedirs(os.path.dirname(debug_pose_path) or ".", exist_ok=True)
        pose_control.save(debug_pose_path)

    # æ‰©å±• person_mask ä¸€ç‚¹ï¼Œç¡®ä¿è¦†ç›–å®Œæ•´
    person_np = np.array(person_mask)
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (20, 20))
    person_np = cv2.dilate(person_np, kernel, iterations=1)
    person_mask_expanded = Image.fromarray(person_np)

    print(f"Stage3: OpenPose ControlNet ä¿®æ­£äººç‰© {w}x{h}...")
    result = pipe_control(
        prompt=prompt,
        negative_prompt=negative_prompt,
        image=base_image,
        mask_image=person_mask_expanded,
        control_image=pose_control,
        controlnet_conditioning_scale=controlnet_conditioning_scale,
        guidance_scale=guidance_scale,
        num_inference_steps=num_inference_steps,
        strength=strength,
        width=w,
        height=h,
    ).images[0]

    return result


# =========================
# 10. å®Œæ•´å››é˜¶æ®µæµç¨‹
# =========================
def run_four_stage_outpaint(
    index: int,
    pose_getter,
    pipe_inpaint,
    pipe_canny,
    pipe_openpose,
    input_path: str,
    tmp_dir: str,
    save_path_template: str = "./run/res_{}.png",
):
    os.makedirs(tmp_dir, exist_ok=True)

    prompt = "1girl, solo, lying down on a red and white bed with plush pillows and soft sheets, peaceful emotion, dark floor beneath, pink hair styled with red and white ribbons tied into bows, fluffy fox ears perked up, anime style, high quality"

    negative_prompt = "lowres, bad anatomy, worst quality, blurry, photorealistic, 3d render, extra limbs, bad hands, malformed limbs"

    # ========== Stage 0 ==========
    print("\n=== Stage 0: æå– Pose å’Œäººç‰© Mask ===")
    pose_image, person_mask = stage0_extract_pose_and_mask(
        input_path, pose_getter, padding=30
    )

    pose_path = os.path.join(tmp_dir, f"stage0_pose_{index}.png")
    mask_path = os.path.join(tmp_dir, f"stage0_mask_{index}.png")
    pose_image.save(pose_path)
    person_mask.save(mask_path)
    print(f"Pose ä¿å­˜: {pose_path}")
    print(f"Mask ä¿å­˜: {mask_path}")

    # ========== Stage 1 ==========
    print("\n=== Stage 1: Base Inpaint æ‰©å±•èƒŒæ™¯ ===")
    stage1_img, expand_mask, person_mask_canvas, pose_canvas = stage1_base_inpaint(
        pipe_inpaint,
        input_path,
        pose_image,
        person_mask,
        prompt=prompt,
        negative_prompt=negative_prompt,
        feather_radius=12,
        guidance_scale=7.5,
        num_inference_steps=30,
        strength=0.85,
    )

    stage1_path = os.path.join(tmp_dir, f"stage1_inpaint_{index}.png")
    stage1_img.save(stage1_path)
    print(f"Stage1 ä¿å­˜: {stage1_path}")

    # ========== Stage 2 ==========
    print("\n=== Stage 2: Canny é‡æ„èƒŒæ™¯ ===")
    stage2_img = stage2_canny_background(
        pipe_canny,
        base_image=stage1_img,
        expand_mask=expand_mask,
        person_mask=person_mask_canvas,
        prompt=prompt,
        negative_prompt=negative_prompt,
        controlnet_conditioning_scale=0.65,
        guidance_scale=6.0,
        num_inference_steps=35,
        strength=0.82,
        canny_low=120,
        canny_high=220,
        canny_blur_sigma=0.8,
        debug_canny_path=os.path.join(tmp_dir, f"stage2_canny_{index}.png"),
    )

    stage2_path = os.path.join(tmp_dir, f"stage2_result_{index}.png")
    stage2_img.save(stage2_path)
    print(f"Stage2 ä¿å­˜: {stage2_path}")

    # ========== Stage 3 ==========
    print("\n=== Stage 3: OpenPose ä¿®æ­£äººç‰© ===")
    final_img = stage3_openpose_refine_person(
        pipe_openpose,
        base_image=stage2_img,
        person_mask=person_mask_canvas,
        pose_control=pose_canvas,
        prompt=prompt,
        negative_prompt=negative_prompt,
        controlnet_conditioning_scale=0.80,
        guidance_scale=6.5,
        num_inference_steps=40,
        strength=0.50,
        debug_pose_path=os.path.join(tmp_dir, f"stage3_pose_{index}.png"),
    )

    save_path = save_path_template.format(index)
    final_img.save(save_path)
    print(f"\nâœ… å®Œæˆç¬¬ {index} æ¬¡ç”Ÿæˆ: {save_path}\n")


# =========================
# 11. ä¸»å‡½æ•°
# =========================
def main():
    cfg = get_default_config()

    animagine_model = cfg.animagine_xl
    base_model = cfg.diffusers_stable_diffusion_xl_inpainting_model
    ctl_model_canny = cfg.control_model_canny
    ctl_model_openpose = cfg.control_model_openpose
    vae_model = cfg.vae_model
    input_img_path = "/home/ping/src/my_python/run/hongxue.jpg"

    print("ğŸš€ æ­£åœ¨å‡†å¤‡æ¨¡å‹è·¯å¾„å’Œèµ„æº...")
    print("animagine_model:", animagine_model)
    print("base_model:", base_model)
    print("ctl_model_canny:", ctl_model_canny)
    print("ctl_model_openpose:", ctl_model_openpose)
    print("vae_model:", vae_model)
    print("input_img_path:", input_img_path)

    print("\nğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹...")

    # Pose æ£€æµ‹å™¨
    print("ğŸ“Œ åŠ è½½ Pose æ£€æµ‹å™¨...")
    pose_getter = RtmlibPoseGet()

    # Stage 1: Base Inpaint
    print("ğŸ“Œ åŠ è½½ Base Inpaint Pipeline...")
    pipe_inpaint = load_inpaint_pipe(base_model, vae_model)

    # Stage 2: Canny ControlNet
    print("ğŸ“Œ åŠ è½½ Canny ControlNet...")
    pipe_canny = load_controlnet_inpaint_pipe(
        animagine_model, ctl_model_canny, vae_model
    )

    # Stage 3: OpenPose ControlNet
    print("ğŸ“Œ åŠ è½½ OpenPose ControlNet...")
    pipe_openpose = load_controlnet_inpaint_pipe(
        animagine_model, ctl_model_openpose, vae_model
    )

    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ\n")

    # è¿è¡Œ 5 æ¬¡ç”Ÿæˆ
    for i in range(1, 6):
        print(f"\n{'=' * 60}")
        print(f"ğŸ¨ å¼€å§‹ç¬¬ {i} æ¬¡å››é˜¶æ®µæ‰©å›¾")
        print(f"{'=' * 60}")

        run_four_stage_outpaint(
            i,
            pose_getter,
            pipe_inpaint,
            pipe_canny,
            pipe_openpose,
            input_img_path,
            tmp_dir="./tmp",
            save_path_template=f"./run/res_{i}.png",
        )


if __name__ == "__main__":
    shutil.rmtree("./tmp", ignore_errors=True)
    main()

    if input("æ˜¯å¦åˆ é™¤ä¸´æ—¶æ–‡ä»¶ï¼Ÿ(y/n): ") == "y":
        shutil.rmtree("./tmp", ignore_errors=True)
